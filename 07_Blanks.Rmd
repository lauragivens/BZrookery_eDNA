---
title: "R Notebook"
output: html_document
---

# Load  
Load libraries  
```{r,message=FALSE}
library(phyloseq)
library(tidyverse)
library(ggplot2)
library(cowplot)
library(vegan)
library(decontam)
```

Load data  
```{r load}
dir_data <- '/Users/lauragivens/Desktop/R/BZrookery_eDNA/Rdata'
dir_results <- "/Volumes/Fuji/Mangroves/2025_0319_Givens_Canty_Rookery_COI/cutadapt/results"

ps <- readRDS(paste0(dir_results,"/ps.rds"))
ps.troph <- readRDS(paste0(dir_results,"/ps.troph.rds"))

taxa <- readRDS(paste0(dir_results,"/taxtable.rds"))
taxa_troph <- readRDS(paste0(dir_results,"/taxtable.wtroph.rds"))
curated_lulu <- readRDS(paste0(dir_results,'/lulu-clustertable.rds'))
curated_asv <- readRDS(paste0(dir_results,"/asvtable.rds"))
samplelist <- readRDS(paste0(dir_results,'/metadata.rds'))
```
Alternatively, load working environment:  
```{r load_env}
load(paste0(dir_data,'/07_Blanks.RData'))
```

```{r}
troph_edit <- taxa_troph %>% mutate(.,
                      Trophic.Base = word(Trophic.Index,sep=" ") %>% as.numeric(),
                      Trophic.Round = round(as.numeric(Trophic.Base)),
                      .after = Trophic.Index
                      ) 

tax_table(ps.troph) <- as.matrix(troph_edit)

samplelist[samplelist==""] <- NA
samplelist[samplelist=="."] <- NA
samplelist$Date <- lubridate::dmy(samplelist$Date)
samplelist$Quant_nguL <- as.numeric(samplelist$Quant_nguL) %>% round(digits = 2)
samplelist$Quant_nguL[is.na(samplelist$Quant_nguL)] <- 0
samplelist$Rookery[is.na(samplelist$Rookery)] <- "Blank"
#samplelist$Month <- lubridate::month(samplelist$Date)
samplelist <- samplelist %>% mutate(.,
                                    Rookery=case_when(Rookery=="R" ~ "Rookery",
                                                      Rookery=="NR" ~ "Non-rookery",
                                                      Rookery=="Blank" ~ "Blank",
                                                      Rookery=="Practice" ~ "Practice"),
                                    Site=case_when(Site=="HI" ~ "Hicks Caye",
                                                   Site=="TA" ~ "Turneffe Atoll",
                                                   Site=="DC" ~ "Drowned Caye",
                                                   Site=="Practice" ~ "Practice",
                                                   Site=="Extraction" ~ "Extraction",
                                                   Site=="Field" ~ "Field",
                                                   Site=="Lab" ~ "Lab"),
                                    Side=case_when(Side=="L" ~ "Leeward",
                                                   Side=="W" ~ "Windward")
                                    )

sample_data(ps.troph) <- samplelist
```

# Blanks  

Make a phyloseq object of only the blanks. How many sequences are in each? 
```{r}
ps.blanks <- ps.troph %>% subset_samples(.,Rookery=='Blank') %>% prune_taxa(taxa_sums(.)>0,.)

blank.asvs <- taxa_names(ps.blanks)

ps.blanks 
sample_sums(ps.blanks) %>% sort()
```

## Percent of sequences/ASVs  
How many ASVs are in the blanks?  
```{r}
length(blank.asvs)
```
What percent of taxa do those ASVs comprise?  
```{r}
ntaxa(prune_taxa(taxa_names(ps)%in%blank.asvs,ps)) / ntaxa(ps) * 100
```
5.43% of taxa    

What percent of sequences do those ASVs comprise?  
```{r}
sum(taxa_sums(prune_taxa(taxa_names(ps)%in%blank.asvs,ps))) / sum(taxa_sums(ps)) * 100
```
71.15% of the sequences  

And how many of those are in the blank samples themselves?  
```{r}
sum(taxa_sums(ps.blanks)) / sum(taxa_sums(ps)) * 100

```
<1% of ASVs are found in the blanks

We want to identify what taxa are in the blanks, and whether it's better to remove all blank ASVs from the dataset or do some other method.  

## What taxa are in blanks  
Are these useful/biologically relevant taxa? Or are they contaminants like human, feline, canine DNA?   
```{r}
get_taxa_unique(ps.blanks,"Species")
```
Many of these are fisheries-relevant or otherwise target fish species.  

So before removing them, we should take a look at what the best way to remove them would be.   

## Compare blanks with high/low sample sums  
Is there much of a difference in ASVs that are found in blanks with a higher or lower number of sequences?   
```{r}
x <- prune_samples(sample_sums(ps.blanks)<100, #get names of asvs in blanks with less than 100 sequences  
                   ps.blanks) %>% 
  prune_taxa(taxa_sums(.)>0,.) %>% 
  taxa_names() 

y <- prune_samples(sample_sums(ps.blanks)>100, #get names of asvs in blanks with more than 100 sequences  
                   ps.blanks) %>% 
  prune_taxa(taxa_sums(.)>0,.) %>% 
  taxa_names() 

y[!y%in%x] #how many asvs are only in the >100 dataset
x[!x%in%y] #how many asvs are only in the <100 dataset  
```

Not really. Looking below, when we prune the dataset to only the blank ASVs, the spread is pretty similar regardless of whether they are from blanks with high or low abundance.  
```{r}
sample_sums(ps.troph %>% prune_taxa(taxa_names(.)%in%x,.)) %>% summary()
sample_sums(ps.troph %>% prune_taxa(taxa_names(.)%in%y,.)) %>% summary()
```
compared to the blanks:  
```{r}
sample_sums(ps.blanks) %>% summary()
```
We are losing some taxa with pretty high abundances if we were to remove all ASVs found in the blanks.  

## Compare taxa before and after blank removal   
When we remove the ASVs from the blanks, it only removes the specific sequence. There could be other ASVs that are assigned to the same taxa, which would make losing those high-abundance ASVs more palatable. Here we are going to get the list of species found in the blanks, and compare sample sums of those species before and after removal. If the sample sums remain high/similar, then removing the specific ASVs found in the blanks are unlikely to have strong negative effects.   

However, this isn't what we find. Removal of blank ASVs isn't compensated for with other ASVs, and the number of sequences still assigned to those taxa drops dramatically.     
```{r}
ps.troph %>%  #sample sums when subset to only species in the blanks
  subset_taxa(.,Species%in%(get_taxa_unique(ps.blanks,"Species"))) %>% 
  sample_sums() %>% summary()

(ps.troph %>% prune_taxa(!taxa_names(.)%in%taxa_names(ps.blanks),.)) %>%  #sample sums when subset to only species in the blanks
  subset_taxa(.,Species%in%(get_taxa_unique(ps.blanks,"Species"))) %>% 
  sample_sums() %>% summary()
```

# Decontamination  

## Identify contaminants  
Okay, so we have decided that carte blanche removing the ASVs that are found in our blanks would result in:  
1. *Removing important taxa* (species assigned to those ASVs are fish)  
2. *Seriously reducing the occurrence of key species in the samples* (other ASVs are not assigned to the same fish species with similar frequency)  

And indeed this is a strategy that is generally seen as overly cautious and likely to remove biologically relevant data. (e.g., QIIME2 documentation, Nguyen et al., 2015 )

We could remove the number of sequences found in the blanks from each sample (again, see Nguyen et al., 2015)
Or remove any ASVs below a certain relative abundance threshold (see background in Davis et al., 2018)
Or use decontam R package, which uses the following principles:   
Sequences from contaminating taxa are likely to have frequencies that inversely correlate with sample DNA concentration [8, 14, 16, 30] and (2) sequences from contaminating taxa are likely to have higher prevalence in control samples than in true samples   

So using decontam package...    
_Using the “frequency” method_, the distribution of the frequency of each sequence feature as a function of the input DNA concentration is used to identify contaminants.  
_Using the “prevalence” method_, the prevalence (presence/absence across samples) of each sequence feature in true positive samples is compared to the prevalence in negative controls to identify contaminants.  

We’ll summarize the sample variable that holds the negative control information as a logical variable, with TRUE for control samples, as that is the form required by isContaminant.  
```{r}
ps.decontam <- ps.troph %>% subset_samples(Site != "Practice")
sample_data(ps.decontam)$is.neg <- sample_data(ps.decontam)$Site == "Field"
sample_data(ps.decontam)$is.neg <- sample_data(ps.decontam)$Site == "Lab"
sample_data(ps.decontam)$is.neg <- sample_data(ps.decontam)$Site == "Extraction"

sample_data(ps.decontam)$Quant_nguL[sample_data(ps.decontam)$Quant_nguL==0] <- 0.000000001
# don't want to include the PCR blanks  
```
```{r}
contamdf.freq <- isContaminant((ps.decontam %>% subset_samples(Quant_nguL>0)), method="frequency", conc="Quant_nguL")
contamdf.prev <- isContaminant(ps.decontam, method="prevalence", neg="is.neg")
```

#### Decontam: Concentration-based  
What contaminants were identified based on DNA concentration?   
```{r}
contamdf.freq %>% filter(contaminant==TRUE)
contamnames.freq <- rownames(contamdf.freq %>% filter(contaminant==TRUE))
```
What taxa are those assigned to?  
```{r}
ps.decontam %>% prune_taxa(taxa_names(.)%in%contamnames.freq,.) %>% tax_table() %>% as.matrix() %>% as.data.frame() %>% .[2:8]
```

What does that look like?  
```{r}
plot_frequency((ps.decontam %>% subset_samples(Quant_nguL>0)), 
               contamnames.freq, conc="Quant_nguL") + 
  xlab("DNA Concentration (ng/uL)")
```
"In this plot the dashed black line shows the model of a noncontaminant sequence feature for which frequency is expected to be independent of the input DNA concentration. The red line shows the model of a contaminant sequence feature, for which frequency is expected to be inversely proportional to input DNA concentration, as contaminating DNA will make up a larger fraction of the total DNA in samples with very little total DNA. "  
So a falsely identified contaminant would not follow the red line very well  

#### Decontam: Prevalence-based  
What contaminants were identified based on prevalence?   
```{r}
contamnames.prev <- rownames(contamdf.prev %>% filter(contaminant==TRUE))
contamdf.prev %>% filter(contaminant==TRUE)
```
What taxa do those correspond to?  
```{r}
ps.decontam %>% prune_taxa(taxa_names(.)%in%contamnames.prev,.) %>% tax_table() %>% as.matrix() %>% as.data.frame() %>% .[2:8]
```

Visualize how many times sequences identified as contaminants were found in negatives versus positives  
```{r}
ps.decontam.pa <- transform_sample_counts(ps.decontam, function(abund) 1*(abund>0))
ps.decontam.pa.neg <- prune_samples(sample_data(ps.decontam.pa)$Rookery == "Blank", ps.decontam.pa)
ps.decontam.pa.pos <- prune_samples(sample_data(ps.decontam.pa)$Rookery != "Blank", ps.decontam.pa)
# Make data.frame of prevalence in positive and negative samples
df.pa <- data.frame(pa.pos=taxa_sums(ps.decontam.pa.pos), pa.neg=taxa_sums(ps.decontam.pa.neg),
                      contaminant=contamdf.prev$contaminant)
ggplot(data=df.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point(position='jitter') +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")
```


# Removal  

## Contaminants  
We are first going to use a combination of both decontam methods to remove contaminant ASVs from the ps object  
(method=combined,minimum, or both)
Combined = probabilities of both are combined with Fisher's method to identify contaminants
Minimum = minimum of both probabilites is used to identify contaminants
Either = contaminants are called if they are identified by either method  
Both = contaminants are called if they are identified by both methods  
```{r}
contamdf <- isContaminant(ps.decontam, method="combined", neg="is.neg", conc="Quant_nguL")
contamdf.names <- row.names(contamdf %>% filter(contaminant==TRUE))
contamdf %>% filter(contaminant==TRUE)
```

What was kept from the separate versions?  
Prevalence: 
```{r}
contamdf.names[contamdf.names%in%contamnames.prev]
```
Frequency: 
```{r}
contamdf.names[contamdf.names%in%contamnames.freq]
```

Curious if the one that was kept was the ASV with really high prevalence in the negative controls  
```{r}
df.comb.pa <- data.frame(pa.pos=taxa_sums(ps.decontam.pa.pos), 
                         pa.neg=taxa_sums(ps.decontam.pa.neg),
                         contaminant=contamdf$contaminant) #new data frame with the combined contaminants included

check <- contamdf %>% select(contaminant) 
head(check)
check$contaminant[rownames(check)!="sq2489"] <- FALSE #only keep the one that was kept from prevanlence  

ggplot(data=df.comb.pa, aes(x=pa.neg, y=pa.pos, color=check$contaminant)) + geom_point(position='jitter') +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")
```
Weirdly enough, no that wasn't the one that decontam decided was a contaminant.  

Just for another double check, what does the plot look like for the combined decontam df?  
```{r}
ggplot(data=df.comb.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point(position='jitter') +
  xlab("Prevalence (Negative Controls)") + ylab("Prevalence (True Samples)")
```


What percent of taxa do those ASVs comprise?  
```{r}
length(contamdf.names) / ntaxa(ps) * 100
```
0.603% of taxa     

What percent of sequences do those ASVs comprise?  
```{r}
sum(taxa_sums(prune_taxa(taxa_names(ps)%in%contamdf.names,ps))) / sum(taxa_sums(ps)) * 100
```
0.29% of the sequences  

```{r}
sample_sums(ps.troph %>% prune_taxa(taxa_names(.)%in%contamdf.names,.)) %>% summary()
```

Remove samples with less than 
```{r}
decontam.ps.troph <- ps.troph %>% 
  #prune_samples(sample_sums(.)>50,.) %>% 
  prune_taxa(!taxa_names(.)%in%contamdf.names,.)#%>% 
  #subset_samples(., Rookery != 'Practice') 

ps.troph
decontam.ps.troph
```
## Blanks  
After decontamination, we are next going to get a list of ASVs that appear in the blanks and their quantities, and remove that number from each sample from that site.    
```{r}
ps.blanks <- decontam.ps.troph %>% subset_samples(.,Rookery=='Blank') %>% prune_taxa(taxa_sums(.)>0,.)
```

We have field blanks, lab blanks, extraction blanks, and PCR blanks.  
```{r}
ps.blanks 
ntaxa(ps.blanks)
sample_sums(ps.blanks) %>% sort()
```
Remove ASVs from PCR blanks from all samples  
PCR blanks:  
```{r}
ps.blanks %>% subset_samples(is.na(Site)) %>% prune_taxa(taxa_sums(.)>0,.) %>% otu_table()
```

Remove ASVs from extraction blanks from their respective extraction day  
```{r}
ps.blanks %>% subset_samples(Site=="Extraction")  %>% prune_taxa(taxa_sums(.)>0,.) %>% otu_table()
```

Remove ASVs from field and lab blanks from their respective sample days   
```{r}
ps.blanks %>% subset_samples(Site=="Lab" | Site=="Field")  %>% prune_taxa(taxa_sums(.)>0,.) %>% otu_table()
```
```{r}
ps.blanks %>% subset_samples(Site=="Lab" | Site=="Field") %>% sample_names()
```


Going in order:  
```{r}
prune.decontam.ps.troph <- decontam.ps.troph
```

### Field and lab blanks   
```{r}
sample_events <- ps.blanks %>% subset_samples(Site=="Lab" | Site=="Field") %>% get_variable(.,"Date") %>% unique()
```
Test:
```{r,eval=FALSE}
 a <- prune.decontam.ps.troph %>% subset_samples(Date==sample_events[1] & Rookery != "Blank") %>% sample_names() #get sample names from that sampling day  
  c <- prune.decontam.ps.troph %>% subset_samples(Date==sample_events[1] & Rookery == "Blank") %>% sample_names() #get sample names of blanks from that trip 
  
  #before:
  #otudf[b,a]
  #blanks:
  #otudf[b,c]
  #after
  #otudf[b,a] %>% mutate(across(all_of(a),~.x - otudf[b,c])) %>% pmax(.,0) 
  

#for (i in 1:length(sample_events)) { #for each sampling day 
#  a <- prune.decontam.ps.troph %>% subset_samples(Date==sample_events[i] & Rookery != "Blank") 
#  print(a)
#}


for (p in 1:length(c)) { #for each negative control sample
   
  b <- prune_samples(sample_names(prune.decontam.ps.troph)==c[p], #get only the blank sample we're working with
                     prune.decontam.ps.troph) %>% 
    prune_taxa(taxa_sums(.)>0,.) %>% taxa_names() #get seqid of asvs in neg control 

  if (length(b)>0){
  for (i in 1:length(b)) { #for each seq in the neg control 
    
     otudf <- otu_table(prune.decontam.ps.troph) %>% as.matrix() %>% as.data.frame()
     otudf[b[i],a] <- otudf[b[i],a] %>% mutate(across(all_of(a),~.x - otudf[b[i],c[p]])) %>% pmax(.,0)
     otu_table(prune.decontam.ps.troph) <- otu_table(as.matrix(otudf),taxa_are_rows = TRUE)
  }
  }
}

#in neg
otu_table(decontam.ps.troph)[b[1],c]

#before
otu_table(decontam.ps.troph)[b[1],a] #samples from that sampling event

#after
otu_table(prune.decontam.ps.troph)[b[1],a]
```

```{r}
for (n in 1:length(sample_events)) { #for each sampling day 
 
  a <- prune.decontam.ps.troph %>% subset_samples(Date==sample_events[n] & Rookery != "Blank") %>% sample_names() #get sample names from that sampling day  
  c <- prune.decontam.ps.troph %>% subset_samples(Date==sample_events[n] & Rookery == "Blank") %>% sample_names()

for (p in 1:length(c)) { #for each negative control sample
   
  b <- prune_samples(sample_names(prune.decontam.ps.troph)==c[p], #get only the blank sample we're working with
                     prune.decontam.ps.troph) %>% 
    prune_taxa(taxa_sums(.)>0,.) %>% taxa_names() #get seqid of asvs in neg control 

  if (length(b)>0){
  for (i in 1:length(b)) { #for each seq in the neg control 
    
     otudf <- otu_table(prune.decontam.ps.troph) %>% as.matrix() %>% as.data.frame()
     otudf[b[i],a] <- otudf[b[i],a] %>% mutate(across(all_of(a),~.x - otudf[b[i],c[p]])) %>% pmax(.,0)
     otu_table(prune.decontam.ps.troph) <- otu_table(as.matrix(otudf),taxa_are_rows = TRUE)
  }
  }
}
  
}

```
Check
```{r}
#in neg
otu_table(decontam.ps.troph)[b[1],c]

#before
otu_table(decontam.ps.troph)[b[1],a] #samples from that sampling event

#after
otu_table(prune.decontam.ps.troph)[b[1],a]
```


# Session Info   
```{r}
saveRDS(decontam.ps.troph,paste0(dir_results,"/decontam.ps.troph.rds"))
```

```{r}
save.image(paste0(dir_data,'/07_Blanks.RData'))
```

```{r}
sessionInfo()
```

